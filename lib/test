[[ $TEST_INCLUDED ]] && return
TEST_INCLUDED=true

# Source dependencies
source "${BASE_DIR}/lib/logger" 2>/dev/null || true

# Test framework configuration
TEST_DIR="${BASE_DIR}/tests"
TEST_RESULTS=()
TEST_PASSED=0
TEST_FAILED=0
TEST_SKIPPED=0
TEST_CURRENT=""

# Test result structure: "status:name:message"
TEST_STATUS_PASS="PASS"
TEST_STATUS_FAIL="FAIL"
TEST_STATUS_SKIP="SKIP"

# Colors for test output
readonly TEST_GREEN='\033[0;32m'
readonly TEST_RED='\033[0;31m'
readonly TEST_YELLOW='\033[1;33m'
readonly TEST_BLUE='\033[0;34m'
readonly TEST_NC='\033[0m'

# Test assertion functions
function assert_equals() {
    local expected="$1"
    local actual="$2"
    local message="${3:-Values should be equal}"
    
    if [[ "$expected" == "$actual" ]]; then
        test_pass "$message"
        return 0
    else
        test_fail "$message (expected: '$expected', actual: '$actual')"
        return 1
    fi
}

function assert_not_equals() {
    local expected="$1"
    local actual="$2"
    local message="${3:-Values should not be equal}"
    
    if [[ "$expected" != "$actual" ]]; then
        test_pass "$message"
        return 0
    else
        test_fail "$message (both values: '$expected')"
        return 1
    fi
}

function assert_true() {
    local condition="$1"
    local message="${2:-Condition should be true}"
    
    if eval "$condition"; then
        test_pass "$message"
        return 0
    else
        test_fail "$message (condition: '$condition')"
        return 1
    fi
}

function assert_false() {
    local condition="$1"
    local message="${2:-Condition should be false}"
    
    if ! eval "$condition"; then
        test_pass "$message"
        return 0
    else
        test_fail "$message (condition: '$condition')"
        return 1
    fi
}

function assert_file_exists() {
    local file="$1"
    local message="${2:-File should exist}"
    
    if [[ -f "$file" ]]; then
        test_pass "$message: $file"
        return 0
    else
        test_fail "$message: $file"
        return 1
    fi
}

function assert_file_not_exists() {
    local file="$1"
    local message="${2:-File should not exist}"
    
    if [[ ! -f "$file" ]]; then
        test_pass "$message: $file"
        return 0
    else
        test_fail "$message: $file"
        return 1
    fi
}

function assert_dir_exists() {
    local dir="$1"
    local message="${2:-Directory should exist}"
    
    if [[ -d "$dir" ]]; then
        test_pass "$message: $dir"
        return 0
    else
        test_fail "$message: $dir"
        return 1
    fi
}

function assert_command_success() {
    local command="$1"
    local message="${2:-Command should succeed}"
    
    if eval "$command" >/dev/null 2>&1; then
        test_pass "$message: $command"
        return 0
    else
        test_fail "$message: $command"
        return 1
    fi
}

function assert_command_failure() {
    local command="$1"
    local message="${2:-Command should fail}"
    
    if ! eval "$command" >/dev/null 2>&1; then
        test_pass "$message: $command"
        return 0
    else
        test_fail "$message: $command"
        return 1
    fi
}

function assert_contains() {
    local haystack="$1"
    local needle="$2"
    local message="${3:-String should contain substring}"
    
    if [[ "$haystack" == *"$needle"* ]]; then
        test_pass "$message"
        return 0
    else
        test_fail "$message (haystack: '$haystack', needle: '$needle')"
        return 1
    fi
}

function assert_not_contains() {
    local haystack="$1"
    local needle="$2"
    local message="${3:-String should not contain substring}"
    
    if [[ "$haystack" != *"$needle"* ]]; then
        test_pass "$message"
        return 0
    else
        test_fail "$message (haystack: '$haystack', needle: '$needle')"
        return 1
    fi
}

# Test lifecycle functions
function test_setup() {
    # Override in test files for setup
    return 0
}

function test_teardown() {
    # Override in test files for cleanup
    return 0
}

function test_before_each() {
    # Override in test files for per-test setup
    return 0
}

function test_after_each() {
    # Override in test files for per-test cleanup
    return 0
}

# Test result recording
function test_pass() {
    local message="$1"
    TEST_RESULTS+=("$TEST_STATUS_PASS:$TEST_CURRENT:$message")
    ((TEST_PASSED++))
    echo -e "${TEST_GREEN}✓${TEST_NC} $TEST_CURRENT: $message"
}

function test_fail() {
    local message="$1"
    TEST_RESULTS+=("$TEST_STATUS_FAIL:$TEST_CURRENT:$message")
    ((TEST_FAILED++))
    echo -e "${TEST_RED}✗${TEST_NC} $TEST_CURRENT: $message"
}

function test_skip() {
    local message="$1"
    TEST_RESULTS+=("$TEST_STATUS_SKIP:$TEST_CURRENT:$message")
    ((TEST_SKIPPED++))
    echo -e "${TEST_YELLOW}⊘${TEST_NC} $TEST_CURRENT: $message"
}

# Test runner functions
function run_test() {
    local test_name="$1"
    local test_function="$2"
    
    TEST_CURRENT="$test_name"
    echo -e "${TEST_BLUE}Running test:${TEST_NC} $test_name"
    
    # Run setup
    test_before_each
    
    # Run the actual test
    if declare -f "$test_function" >/dev/null 2>&1; then
        "$test_function"
    else
        test_fail "Test function not found: $test_function"
    fi
    
    # Run cleanup
    test_after_each
    
    echo
}

function run_test_suite() {
    local test_file="$1"
    
    if [[ ! -f "$test_file" ]]; then
        log_error "Test file not found: $test_file"
        return 1
    fi
    
    echo -e "${TEST_BLUE}Running test suite:${TEST_NC} $(basename "$test_file")"
    echo "================================"
    
    # Initialize counters
    TEST_PASSED=0
    TEST_FAILED=0
    TEST_SKIPPED=0
    TEST_RESULTS=()
    
    # Source the test file
    source "$test_file"
    
    # Run setup
    test_setup
    
    # Find and run all test functions
    local test_functions
    test_functions=$(declare -F | grep "declare -f test_" | cut -d' ' -f3 | grep -v "test_setup\|test_teardown\|test_before_each\|test_after_each")
    
    for test_func in $test_functions; do
        local test_name
        test_name=$(echo "$test_func" | sed 's/test_//' | sed 's/_/ /g')
        run_test "$test_name" "$test_func"
    done
    
    # Run teardown
    test_teardown
    
    # Print summary
    print_test_summary
}

function run_all_tests() {
    if [[ ! -d "$TEST_DIR" ]]; then
        log_warn "Test directory not found: $TEST_DIR"
        return 0
    fi
    
    echo -e "${TEST_BLUE}Running all tests in:${TEST_NC} $TEST_DIR"
    echo "========================================"
    
    local total_passed=0
    local total_failed=0
    local total_skipped=0
    
    for test_file in "$TEST_DIR"/*.sh; do
        if [[ -f "$test_file" ]]; then
            run_test_suite "$test_file"
            
            ((total_passed += TEST_PASSED))
            ((total_failed += TEST_FAILED))
            ((total_skipped += TEST_SKIPPED))
            
            echo
        fi
    done
    
    # Print overall summary
    echo "========================================"
    echo -e "${TEST_BLUE}Overall Test Results:${TEST_NC}"
    echo -e "  ${TEST_GREEN}Passed:${TEST_NC}  $total_passed"
    echo -e "  ${TEST_RED}Failed:${TEST_NC}  $total_failed"
    echo -e "  ${TEST_YELLOW}Skipped:${TEST_NC} $total_skipped"
    echo -e "  ${TEST_BLUE}Total:${TEST_NC}   $((total_passed + total_failed + total_skipped))"
    
    if [[ $total_failed -gt 0 ]]; then
        echo -e "${TEST_RED}Some tests failed!${TEST_NC}"
        return 1
    else
        echo -e "${TEST_GREEN}All tests passed!${TEST_NC}"
        return 0
    fi
}

function print_test_summary() {
    local total=$((TEST_PASSED + TEST_FAILED + TEST_SKIPPED))
    
    echo "Test Summary:"
    echo "============="
    echo -e "  ${TEST_GREEN}Passed:${TEST_NC}  $TEST_PASSED"
    echo -e "  ${TEST_RED}Failed:${TEST_NC}  $TEST_FAILED"
    echo -e "  ${TEST_YELLOW}Skipped:${TEST_NC} $TEST_SKIPPED"
    echo -e "  ${TEST_BLUE}Total:${TEST_NC}   $total"
    
    if [[ $TEST_FAILED -gt 0 ]]; then
        echo
        echo -e "${TEST_RED}Failed tests:${TEST_NC}"
        for result in "${TEST_RESULTS[@]}"; do
            IFS=':' read -r status name message <<< "$result"
            if [[ "$status" == "$TEST_STATUS_FAIL" ]]; then
                echo -e "  ${TEST_RED}✗${TEST_NC} $name: $message"
            fi
        done
    fi
}

# Test utilities
function create_temp_test_file() {
    local content="$1"
    local temp_file
    temp_file=$(mktemp)
    echo "$content" > "$temp_file"
    echo "$temp_file"
}

function create_temp_test_dir() {
    mktemp -d
}

function cleanup_test_files() {
    # Clean up any test files created during testing
    find /tmp -name "test_*" -type f -mtime +1 -delete 2>/dev/null || true
    find /tmp -name "dotfiles_test_*" -type d -mtime +1 -exec rm -rf {} + 2>/dev/null || true
}

# Mock functions for testing
function mock_command() {
    local command="$1"
    local mock_output="$2"
    local mock_exit_code="${3:-0}"
    
    # Create a mock script
    local mock_script="/tmp/mock_$command"
    cat > "$mock_script" << EOF
#!/bin/bash
echo "$mock_output"
exit $mock_exit_code
EOF
    chmod +x "$mock_script"
    
    # Add to PATH
    export PATH="/tmp:$PATH"
    
    echo "Mocked command: $command"
}

function unmock_command() {
    local command="$1"
    rm -f "/tmp/mock_$command"
    echo "Unmocked command: $command"
}

# Performance testing
function benchmark() {
    local description="$1"
    local command="$2"
    local iterations="${3:-1}"
    
    echo "Benchmarking: $description"
    
    local start_time
    local end_time
    local total_time=0
    
    for ((i = 1; i <= iterations; i++)); do
        start_time=$(date +%s.%N)
        eval "$command" >/dev/null 2>&1
        end_time=$(date +%s.%N)
        
        local iteration_time
        iteration_time=$(echo "$end_time - $start_time" | bc -l 2>/dev/null || echo "0")
        total_time=$(echo "$total_time + $iteration_time" | bc -l 2>/dev/null || echo "$total_time")
    done
    
    local average_time
    average_time=$(echo "scale=4; $total_time / $iterations" | bc -l 2>/dev/null || echo "0")
    
    echo "  Iterations: $iterations"
    echo "  Total time: ${total_time}s"
    echo "  Average time: ${average_time}s"
}

# Test discovery
function discover_tests() {
    if [[ -d "$TEST_DIR" ]]; then
        find "$TEST_DIR" -name "*.sh" -type f | sort
    fi
}

function list_test_functions() {
    local test_file="$1"
    
    if [[ -f "$test_file" ]]; then
        source "$test_file"
        declare -F | grep "declare -f test_" | cut -d' ' -f3 | grep -v "test_setup\|test_teardown\|test_before_each\|test_after_each"
    fi
}

# Initialize test framework
log_debug "Test framework initialized"
log_debug "Test directory: $TEST_DIR"

# Clean up old test files
cleanup_test_files 